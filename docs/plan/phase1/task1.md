# Task 1: LLM Router + Vault API Key ê´€ë¦¬ + PDF íŒŒì‹± + 3-ë¶„í•  WebSocket

**Phase**: Week 1 MVP  
**ìš°ì„ ìˆœìœ„**: Highest  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1ì¼ (ê¸ˆìš”ì¼)  
**ë‹´ë‹¹**: Claude Code (êµ¬í˜„) / Kimi (ê²€ì¦)

---

## ğŸ¯ ëª©í‘œ

ì§€ë‚œì£¼ RFP ë¶„ì„ì„ ìœ„í•œ ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•:
1. **ë³´ì•ˆ**: Vaultì—ì„œ API Key ì•ˆì „í•˜ê²Œ ê´€ë¦¬ ë° ë¡œë“œ
2. **ìœ ì—°ì„±**: 3ê°œ Provider (Claude/GPT/Gemini) ë™ì  ë¼ìš°íŒ… + ìë™ ë°±ì—…
3. **ì…ë ¥**: 80-120í˜ì´ì§€ PDF íŒŒì¼ íŒŒì‹± (ë¯¼ê°ì •ë³´ ì—†ëŠ” ë²„ì „)
4. **ì¶œë ¥**: 3ê°œ LLMì´ ë™ì‹œ ë¶„ì„í•  ìˆ˜ ìˆëŠ” WebSocket ì¸í”„ë¼

ì‹œì—° ì‹œë‚˜ë¦¬ì˜¤: "RFPë¥¼ AI 3ëª…ì—ê²Œ ë™ì‹œì— ë¶„ì„ì‹œì¼°ìŠµë‹ˆë‹¤"

---

## ğŸ“‹ êµ¬í˜„ ë²”ìœ„

### 1. Vault API Key ê´€ë¦¬

**ìœ„ì¹˜**: `src/core/settings.py`

```python
import hvac
from pydantic_settings import BaseSettings

class VaultConfig(BaseModel):
    url: str = "http://localhost:8201"
    token: str = ""  # VAULT_TOKEN í™˜ê²½ë³€ìˆ˜
    secret_path: str = "secret/data/ai-chat/llm-keys"

class Settings(BaseSettings):
    app_env: str = "local"
    use_vault: bool = False
    vault: VaultConfig = VaultConfig()
    
    def load_api_keys(self) -> dict[str, str]:
        """Vault ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ"""
        if self.use_vault:
            client = hvac.Client(url=self.vault.url, token=self.vault.token)
            return client.read(self.vault.secret_path)["data"]["data"]
        else:
            return {
                "anthropic": os.getenv("ANTHROPIC_API_KEY"),
                "openai": os.getenv("OPENAI_API_KEY"),
                "google": os.getenv("GOOGLE_API_KEY"),
            }
```

**ì„¤ì • íŒŒì¼**: `config/config-local.yml`
```yaml
vault:
  url: "http://localhost:8201"
  token: "${VAULT_TOKEN}"
  secret_path: "secret/data/ai-chat/llm-keys"

llm:
  primary_provider: "claude"
  backup_chain: ["claude", "openai", "gemini"]
  models:
    claude: "claude-opus-4-5-20251101"
    openai: "gpt-5.1"
    gemini: "gemini-3-pro-preview"
```

**ê²€ì¦ í•­ëª©**:
- [ ] Vault ì—°ê²° ì„±ê³µ (localhost:8201)
- [ ] API í‚¤ ë¡œë“œ ì„±ê³µ (3ê°œ Provider: anthropic, openai, google)
- [ ] ë°±ì—… ì²´ì¸ ìˆœí™˜ ë¡œì§ ì •ìƒ ì‘ë™
- [ ] ì‹¤íŒ¨ ì‹œ ìë™ ì „í™˜ (+ ë¡œê·¸ ê¸°ë¡)

---

### 2. LLM Router (Claude Opus 4.5 ì§€ì›)

**ìœ„ì¹˜**: `src/core/llm_router.py`

```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

PROVIDER_CONFIG = {
    "claude": {
        "model": "claude-opus-4-5-20251101",
        "client_class": ChatAnthropic,
        "api_key_name": "anthropic",
        "cost_per_1k": 0.075
    },
    "openai": {
        "model": "gpt-5.1",
        "client_class": ChatOpenAI,
        "api_key_name": "openai",
        "cost_per_1k": 0.01
    },
    "gemini": {
        "model": "gemini-3-pro-preview",
        "client_class": ChatGoogleGenerativeAI,
        "api_key_name": "google",
        "cost_per_1k": 0.00125
    }
}

class LLMRouter:
    def __init__(self, settings: Settings):
        self.api_keys = settings.load_api_keys()
        self.backup_chain = settings.llm.backup_chain
    
    def get_llm(self, provider: str, use_backup: bool = True):
        """Providerë³„ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± + ë°±ì—… ì¤€ë¹„"""
        try:
            config = PROVIDER_CONFIG[provider]
            api_key = self.api_keys.get(config["api_key_name"])

            return config["client_class"](
                model=config["model"],
                api_key=api_key,
                streaming=True,
                max_tokens=8192,
                temperature=0.2
            )
        except Exception as e:
            if use_backup:
                return self._try_backup(provider)
            raise
    
    def _try_backup(self, failed_provider: str):
        """ë°±ì—… ì²´ì¸ì—ì„œ ë‹¤ìŒ Provider ì‹œë„"""
        idx = self.backup_chain.index(failed_provider)
        for backup in self.backup_chain[idx+1:]:
            try:
                return self.get_llm(backup, use_backup=False)
            except:
                continue
        raise RuntimeError("ëª¨ë“  ë°±ì—… Provider ì‹¤íŒ¨")
```

**Rate Limit ì²˜ë¦¬** (`src/core/retry.py`):
```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from openai import RateLimitError as OpenAIRateLimitError
from anthropic import RateLimitError as AnthropicRateLimitError
from google.api_core.exceptions import ResourceExhausted

# ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
llm_retry = retry(
    retry=retry_if_exception_type((
        OpenAIRateLimitError,
        AnthropicRateLimitError,
        ResourceExhausted
    )),
    wait=wait_exponential(multiplier=1, min=2, max=60),
    stop=stop_after_attempt(3),
    reraise=True
)

# ì‚¬ìš© ì˜ˆì‹œ
class LLMRouter:
    @llm_retry
    async def call_with_retry(self, provider: str, messages: list):
        llm = self.get_llm(provider)
        return await llm.ainvoke(messages)
```

**ê²€ì¦ í•­ëª©**:
- [ ] Claude Opus 4.5 í˜¸ì¶œ ì„±ê³µ
- [ ] ì˜ëª»ëœ í‚¤ë¡œ í…ŒìŠ¤íŠ¸ ì‹œ ë°±ì—… ì „í™˜
- [ ] Rate Limit(60rpm) ì´ˆê³¼ ì‹œ tenacity ì¬ì‹œë„ ë™ì‘
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì •ìƒ ì‘ë™

---

### 3. PDF íŒŒì‹±

**ìœ„ì¹˜**: `src/utils/pdf_parser.py`

```python
import pdfplumber
import re

class PDFParser:
    def __init__(self, max_pages: int = 120):
        self.max_pages = max_pages

    def extract_text(self, file_path: str) -> str:
        """PDF íŒŒì‹±"""
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages[:self.max_pages]:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n\n"

        return text.strip()
    
    def get_metadata(self, file_path: str) -> dict:
        """PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        with pdfplumber.open(file_path) as pdf:
            return {
                "pages": len(pdf.pages),
                "size": pdf.metadata.get("File size", "Unknown")
            }
```

**RFP ìƒ˜í”Œ ì¤€ë¹„**:
```bash
config/samples/
â”œâ”€â”€ rfp_small_30p.pdf      # í…ŒìŠ¤íŠ¸ìš©
â”œâ”€â”€ rfp_medium_80p.pdf     # ì‹œì—°ìš© (ì£¼ë ¥)
â””â”€â”€ rfp_large_120p.pdf     # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
```

**ê²€ì¦ í•­ëª©**:
- [ ] PDF íŒŒì‹± ì •ìƒ ì‘ë™ (80~120í˜ì´ì§€)
- [ ] íŠ¹ìˆ˜ ë¬¸ì/í‘œ ì²˜ë¦¬ ì˜¤ë¥˜ ì—†ìŒ

---

### 4. 3-ë¶„í•  WebSocket

**ìœ„ì¹˜**: `src/api/routes/chat.py`

```python
from fastapi import WebSocket, WebSocketDisconnect
import json
import asyncio

class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}
    
    async def connect(self, provider: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[provider] = websocket
    
    def disconnect(self, provider: str):
        self.active_connections.pop(provider, None)
    
    async def broadcast_progress(self, message: dict):
        """ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì— ì§„í–‰ìƒí™© ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        for connection in self.active_connections.values():
            await connection.send_json(message)

manager = ConnectionManager()

@router.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket, provider: str):
    """
    íŒŒë¼ë¯¸í„°:
    - provider: claude | openai | gemini
    
    ë©”ì‹œì§€ í˜•ì‹:
    {"type": "analyze", "file_id": "uuid", "prompt": "..."}
    {"type": "user_rating", "rating": 4}
    """
    await manager.connect(provider, websocket)
    
    try:
        while True:
            data = await websocket.receive_json()
            
            if data["type"] == "analyze":
                # ë¹„ë™ê¸° ë¶„ì„ ì‹œì‘
                asyncio.create_task(
                    analyze_rfp(provider, data["file_id"], data["prompt"])
                )
            
            elif data["type"] == "user_rating":
                # ë³„ì  ì €ì¥
                state.evaluation.user_ratings[provider] = data["rating"]
    
    except WebSocketDisconnect:
        manager.disconnect(provider)

async def analyze_rfp(provider: str, file_id: str, prompt: str):
    """ì‹¤ì œ LLM ë¶„ì„ ì‹¤í–‰"""
    try:
        # LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        llm = llm_router.get_llm(provider)
        
        # PDF ë‚´ìš© ë¡œë“œ
        file_content = file_store.get(file_id)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        async for chunk in llm.astream(
            f"ë‹¤ìŒ RFPë¥¼ ë¶„ì„í•˜ì„¸ìš”:\n\n{file_content}\n\n{prompt}"
        ):
            await manager.broadcast_progress({
                "provider": provider,
                "status": "streaming",
                "chunk": chunk.content
            })
        
        # ì™„ë£Œ
        await manager.broadcast_progress({
            "provider": provider,
            "status": "complete"
        })
    
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ë°±ì—… ì „í™˜
        backup_llm = llm_router.get_llm(provider, use_backup=True)
        # ... ì¬ì‹œë„ ë¡œì§
```

**ìœ„ì¹˜**: `static/js/chat.js`

```javascript
// 3ê°œ WebSocket ë™ì‹œ ì—°ê²°
const connections = {
    claude: new WebSocket('ws://localhost:8000/ws/chat?provider=claude'),
    openai: new WebSocket('ws://localhost:8000/ws/chat?provider=openai'),
    gemini: new WebSocket('ws://localhost:8000/ws/chat?provider=gemini')
};

// ê° ì—°ê²°ì— ëŒ€í•œ í•¸ë“¤ëŸ¬
Object.keys(connections).forEach(provider => {
    const ws = connections[provider];
    const container = document.getElementById(`${provider}-output`);
    
    ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        
        if (data.status === 'streaming') {
            container.innerHTML += data.chunk;
        } else if (data.status === 'complete') {
            container.classList.add('complete');
            updateScore(provider, data.score);
        }
    };
});

// ë¶„ì„ ì‹œì‘ í•¨ìˆ˜
function startAnalysis() {
    const fileId = document.getElementById('file-id').value;
    
    Object.entries(connections).forEach(([provider, ws]) => {
        ws.send(JSON.stringify({
            type: 'analyze',
            file_id: fileId,
            prompt: 'ì´ RFPì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ 5ê°€ì§€ë¥¼ ë¶„ì„í•˜ê³  ì œì•ˆì„œ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.'
        }));
    });
}

// ë³„ì  ì „ì†¡
function sendRating(provider, rating) {
    connections[provider].send(JSON.stringify({
        type: 'user_rating',
        rating: rating
    }));
}
```

**UI êµ¬ì¡°** (`templates/index.html`):
```html
<div class="grid">
  <div class="column">
    <h3>Claude Opus 4.5</h3>
    <div id="claude-output" class="output"></div>
    <div class="star-rating" data-provider="claude"></div>
  </div>

  <div class="column">
    <h3>GPT-5.1</h3>
    <div id="openai-output" class="output"></div>
    <div class="star-rating" data-provider="openai"></div>
  </div>

  <div class="column">
    <h3>Gemini 3 Pro</h3>
    <div id="gemini-output" class="output"></div>
    <div class="star-rating" data-provider="gemini"></div>
  </div>
</div>
```

**ê²€ì¦ í•­ëª©**:
- [ ] 3ê°œ WebSocket ë™ì‹œ ì—°ê²° ì„±ê³µ
- [ ] í† í°ë³„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤ì‹œê°„ í‘œì‹œ
- [ ] í•œ ê°œ ì‹¤íŒ¨ ì‹œ ë‚˜ë¨¸ì§€ 2ê°œ ì •ìƒ ì‘ë™
- [ ] ë°±ì—… ì „í™˜ ì‹œ UIì— ì•Œë¦¼ í‘œì‹œ
- [ ] ë³„ì  í´ë¦­ ì‹œ ì„œë²„ì— ì •ìƒ ì „ì†¡

---

## ğŸ“¦ ì˜ì¡´ì„± ë° ì„¤ì •

**pyproject.toml**:
```toml
[project]
name = "agent-gaia"
version = "0.1.0"
dependencies = [
    "fastapi[standard]>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "langgraph>=0.2.0",
    "langchain-anthropic>=0.3.0",
    "langchain-openai>=0.2.0",
    "langchain-google-genai>=2.0.0",
    "pydantic-settings>=2.0.0",
    "pyyaml>=6.0",
    "hvac>=2.0.0",              # Vault
    "tenacity>=8.2.0",          # Rate Limit ì¬ì‹œë„
    "pdfplumber>=0.10.0",       # PDF íŒŒì‹±
    "python-docx>=1.1.0",       # Word ìƒì„±
    "python-multipart>=0.0.6",
]

[project.optional-dependencies]
dev = ["pytest>=8.0.0", "pytest-asyncio>=0.21.0"]
```

**í™˜ê²½ ì„¤ì •** (`.env.example`):
```bash
# Vault ì—°ê²° (ì‹œì—° í•„ìˆ˜)
VAULT_TOKEN=myroot
VAULT_URL=http://localhost:8201

# ë¡œì»¬ ê°œë°œìš© (Vault ì—†ì„ ì‹œ)
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
```

**ì‹¤í–‰ ëª…ë ¹ì–´**:
```bash
# ì„¤ì¹˜
uv sync

# ì‹¤í–‰
export VAULT_TOKEN=myroot
uv run run.py --local --use-vault

# ì ‘ì†
open http://localhost:8000
```

---

## âœ… ê²€ì¦ ê¸°ì¤€ (KIMI ê²€ì¦ìš©)

### ê¸°ëŠ¥ ê²€ì¦
- [ ] **Vault**: 3ê°œ API í‚¤ ëª¨ë‘ ë¡œë“œ ì„±ê³µ (anthropic, openai, google)
- [ ] **LLM Router**: Claude Opus 4.5 í˜¸ì¶œ ì„±ê³µ
- [ ] **Backup**: ì˜ëª»ëœ í‚¤ë¡œ í…ŒìŠ¤íŠ¸ ì‹œ ìë™ ì „í™˜
- [ ] **PDF Parser**: PDF íŒŒì‹± ì •ìƒ ì‘ë™
- [ ] **WebSocket**: 3ê°œ ë™ì‹œ ì—°ê²°, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- [ ] **Star Rating**: ë³„ì  í´ë¦­ â†’ ì„œë²„ ì €ì¥ í™•ì¸

### ì„±ëŠ¥ ê²€ì¦
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < 500MB (3ê°œ WebSocket)
- [ ] WebSocket ë©”ì‹œì§€ latency < 100ms

### ë³´ì•ˆ ê²€ì¦
- [ ] API í‚¤ê°€ ë¡œê·¸/ì—ëŸ¬ ë©”ì‹œì§€ì— ë…¸ì¶œë˜ì§€ ì•ŠìŒ
- [ ] .env íŒŒì¼ Git ì¶”ì  ì•ˆë¨ (.gitignore í™•ì¸)
- [ ] ì—…ë¡œë“œëœ PDF ì„ì‹œ ì €ì¥ì†Œ ì•”í˜¸í™” (ì„ íƒì‚¬í•­)

---

## ğŸ¯ ì‹œì—° í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì •ìƒ ì‘ë™
```
1. config/samples/rfp_medium_80p.pdf ì—…ë¡œë“œ
2. 3ê°œ LLM ë™ì‹œ ë¶„ì„ ì‹œì‘
3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™•ì¸ (3ê°œ ë¶„í• )
4. ê° ëª¨ë¸ë³„ ë³„ì  3-5ê°œ ì…ë ¥
5. ë¶„ì„ ì™„ë£Œ í›„ ì ìˆ˜ í™•ì¸
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë°±ì—… ì „í™˜
```
1. Vaultì—ì„œ Claude í‚¤ ì„ì‹œ ì‚­ì œ
2. ì—…ë¡œë“œ í›„ ë¶„ì„ ì‹œì‘
3. UIì— "Claude ì—°ê²° ì‹¤íŒ¨ â†’ GPT-4o ì „í™˜" ì•Œë¦¼ í‘œì‹œ
4. GPT-4oê°€ Claude ì—­í•  ëŒ€ì‹  ìˆ˜í–‰
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ìš©ëŸ‰ PDF
```
1. config/samples/rfp_large_120p.pdf ì—…ë¡œë“œ
2. íŒŒì‹± ë° ë¶„ì„ ì •ìƒ ì‘ë™ í™•ì¸
```

---

## ğŸ” ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘

| ìœ„í—˜ | í™•ë¥  | ì˜í–¥ | ëŒ€ì‘ |
|------|------|------|------|
| Vault ì—°ê²° ì‹¤íŒ¨ | ì¤‘ | ë†’ | ë¡œì»¬ .env ë°±ì—… ëª¨ë“œ ì¤€ë¹„ |
| PDF íŒŒì‹± ì§€ì—° (30ì´ˆ+) | ë‚® | ì¤‘ | ë¯¸ë¦¬ íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ìºì‹œ |
| WebSocket ì—°ê²° ëŠê¹€ | ì¤‘ | ì¤‘ | ìë™ ì¬ì—°ê²° + ì§„í–‰ìƒí™© ë³µì› |
| Rate Limit ì´ˆê³¼ | ì¤‘ | ë†’ | ìš”ì²­ ê°„ 1ì´ˆ ì§€ì—°, ìºì‹œ í™œìš© |
| 3ê°œ ëª¨ë¸ ë™ì‹œ ì¥ì•  | ë‚® | ë†’ | ë¡œì»¬ Ollama ëª¨ë¸ ì¤€ë¹„ |

**ì´ê´„ ê²€ì¦ì**: KIMI
**êµ¬í˜„ ì™„ë£Œ í›„ ê²€ì¦ í•­ëª©**: ìœ„ ê²€ì¦ ê¸°ì¤€ ëª¨ë‘ ì²´í¬

---

**ë‹¤ìŒ ì‘ì—…**: ì´ ì„¤ê³„ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ Claude Codeê°€ êµ¬í˜„ ì‹œì‘  
**ê²€ì¦ ì‹œì **: êµ¬í˜„ ì™„ë£Œ í›„ KIMIê°€ ì½”ë“œ ë¦¬ë·° ë° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰